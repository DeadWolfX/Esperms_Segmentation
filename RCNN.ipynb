{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075aabb-6f97-46f8-9373-0a6bc5543a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchxrayvision as xrv\n",
    "import skimage, torch, torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import io, transform\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "import sys\n",
    "import re\n",
    "from data_aug.data_aug import *\n",
    "from data_aug.bbox_util import *\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb172c-c1e9-47bc-a6cb-f3692ab94c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Visualize(img,bboxes,bb=False,ruta=False,dim=False,bz=1,bboxtipe='coco'):\n",
    "\n",
    "    if ruta:\n",
    "      print('ruta:')\n",
    "      print(r)\n",
    "    if bb:\n",
    "      print('bbox:')\n",
    "      print(bboxes)\n",
    "    if dim:\n",
    "      print('img inf:')\n",
    "      print(type(img))\n",
    "\n",
    "    for i in range(bz):\n",
    "      if bboxtipe=='coco':\n",
    "          bboxes=[bboxes[0].item(),bboxes[1].item(),bboxes[2].item(),bboxes[3].item()]\n",
    "      else:\n",
    "          bboxes=[bboxes[0].item(),bboxes[1].item(),bboxes[2].item()-bboxes[0].item(),bboxes[3].item()-bboxes[1].item()]\n",
    "      fig, ax = plt.subplots(figsize=(18, 5), facecolor='w', edgecolor='b')\n",
    "      ax.imshow(img[i],cmap='gray')\n",
    "      rect = patches.Rectangle((bboxes[0],bboxes[1]),bboxes[2],bboxes[3], linewidth=1.5, edgecolor='r', facecolor='none')\n",
    "      ax.add_patch(rect)\n",
    "\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bdb19a-7f18-423b-b6f9-c62a1933981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d470ab72-b06f-47e1-bbab-516dd9e51112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_bb(bb1,bb2,im1,im2):\n",
    "    bb=[]\n",
    "    if len(bb2)>0:\n",
    "        bb=bb2\n",
    "        im=im2\n",
    "    else:\n",
    "        bb=bb1\n",
    "        im=im1\n",
    "    return im, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d2a9e-f260-4b74-bd00-f3dd9df4d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path, target_size, normalizacion=\"[-1,1]\", type_resize='interpolation', augmentation=None,bboxtipe='coco',dataroot=''):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.norm=normalizacion\n",
    "        self.target_size = target_size\n",
    "        self.type_resize = type_resize\n",
    "        self.augmentation = augmentation\n",
    "        self.bboxtipe = bboxtipe\n",
    "        self.dataroot = dataroot\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def resize_image(self, image):\n",
    "        if  self.type_resize == 'interpolation':\n",
    "          resized_image=skimage.transform.resize(image, self.target_size)\n",
    "        else:\n",
    "          original_height, original_width = image.shape\n",
    "          target_height, target_width = self.target_size\n",
    "\n",
    "          # Calcula las escalas de cambio de tamaño en ambas dimensisones\n",
    "          scale_height = target_height / original_height\n",
    "          scale_width = target_width / original_width\n",
    "\n",
    "          # Elige la escala más pequeña para asegurarte de que la imagen se ajuste en el nuevo tamaño\n",
    "          scale = min(scale_height, scale_width)\n",
    "\n",
    "          # Calcula las nuevas dimensiones después del cambio de tamaño\n",
    "          new_height = int(original_height * scale)\n",
    "          new_width = int(original_width * scale)\n",
    "\n",
    "          # Realiza el cambio de tamaño utilizando el método 'letterboxing'\n",
    "          resized_image = np.zeros((target_height, target_width))\n",
    "          top = (target_height - new_height) // 2\n",
    "          left = (target_width - new_width) // 2\n",
    "          resized_image[top:top + new_height, left:left + new_width] = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "        return resized_image\n",
    "\n",
    "    def resize_bbox(self, bbox, original_shape):\n",
    "        # Redimensionar las coordenadas del bounding box para que se ajusten al nuevo tamañofrom tqdm import tqdm\n",
    "        x, y, width, height = bbox\n",
    "        new_width = width * self.target_size[1] / original_shape[1]\n",
    "        new_height = height * self.target_size[0] / original_shape[0]\n",
    "        new_x = x * self.target_size[1] / original_shape[1]\n",
    "        new_y = y * self.target_size[0] / original_shape[0]\n",
    "\n",
    "        return [new_x, new_y, new_width, new_height]\n",
    "\n",
    "    def apply_aug(self,image,bbox):\n",
    "        if self.augmentation:\n",
    "            bboxr=np.array([[bbox[0],bbox[1],bbox[0]+bbox[2],bbox[1]+bbox[3]]])\n",
    "            imgr = np.stack((image, image, image)).transpose(1, 2, 0)\n",
    "            imgr1, bboxr1=RandomHorizontalFlip(0.5)(imgr, bboxr)\n",
    "            imgr1, bboxr1=verify_bb(bboxr,bboxr1,imgr,imgr1)\n",
    "            imgr2, bboxr2=RandomScale(0.1, diff = True)(imgr1,bboxr1)\n",
    "            imgr2, bboxr2=verify_bb(bboxr1,bboxr2,imgr1,imgr2)\n",
    "            imgr3, bboxr3=RandomTranslate(0.05, diff = True)(imgr2, bboxr2)\n",
    "            imgr3, bboxr3=verify_bb(bboxr2,bboxr3,imgr2,imgr3)\n",
    "            imgr4, bboxr4=RandomRotate(5)(imgr3, bboxr3)\n",
    "            imgr4, bboxr4=verify_bb(bboxr3,bboxr4,imgr3,imgr4)\n",
    "            imgrf, bboxrf=RandomShear(0.1)(imgr4, bboxr4)\n",
    "            imgrf, bboxrf=verify_bb(bboxr4,bboxrf,imgr4,imgrf)\n",
    "            img=np.array(imgrf[...,0]).astype(np.uint8)\n",
    "            bbox=[float(bboxrf[0][0]),float(bboxrf[0][1]),float(bboxrf[0][2]-bboxrf[0][0]),float(bboxrf[0][3]-bboxrf[0][1])]\n",
    "        else:\n",
    "            img=image\n",
    "            bbox=bbox\n",
    "        return img , bbox\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_ruta = self.dataroot+self.data['ruta'][idx]\n",
    "        image = io.imread(img_ruta)\n",
    "        # Extract bounding box coordinates\n",
    "        x, y, width, height = self.data.iloc[idx, 1:5]\n",
    "\n",
    "        image,bbox=self.apply_aug(image,[x,y,width,height])\n",
    "\n",
    "        if self.norm == \"[-1,1]\":\n",
    "          #normaliza a [-1,1]\n",
    "          if image.dtype == np.uint8:  # Si la imagen es de 8 bits\n",
    "              image = (image / 255.0) * 2.0 - 1.0\n",
    "          elif image.dtype == np.uint16:  # Si la imagen es de 16 bits\n",
    "              image = (image / 65535.0) * 2.0 - 1.0\n",
    "          else:\n",
    "              raise ValueError(\"Tipo de datos de imagen no compatible.\")\n",
    "        else:\n",
    "          #Normaliza a [0,1]\n",
    "          if image.dtype == np.uint8:  # Si la imagen es de 8 bits\n",
    "            image = image / 255.0\n",
    "          elif image.dtype == np.uint16:  # Si la imagen es de 16 bits\n",
    "            image = image / 65535.0\n",
    "          else:\n",
    "              raise ValueError(\"Tipo de datos de imagen no compatible.\")\n",
    "\n",
    "        # Redimensionar la imagen y el bounding box\n",
    "        image_r = self.resize_image(image)\n",
    "        bbox_r = self.resize_bbox(bbox, image.shape)\n",
    "        image_r= image_r[np.newaxis, ...]\n",
    "        area = bbox_r[2]*bbox_r[3]\n",
    "        if self.bboxtipe != 'coco':\n",
    "          bbox_r=[bbox_r[0],bbox_r[1],bbox_r[0]+bbox_r[2],bbox_r[1]+bbox_r[3]]\n",
    "        image_r = torch.tensor(image_r, dtype=torch.float32)\n",
    "        bbox_r = torch.tensor([bbox_r], dtype=torch.float32)\n",
    "\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bbox_r\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = torch.tensor([area])\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        return image_r, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495437e-6c98-4715-829c-8fadf4c24aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"/home/jair/\"\n",
    "csv_path_traind = dataroot+\"Anotaciones_estructuradas/deteccion_train.csv\"\n",
    "csv_path_testd = dataroot+\"Anotaciones_estructuradas/deteccion_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce485098-cc37-4f38-9f2c-dc024cb02394",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size=(300,300)\n",
    "normalize=\"[0,1]\"\n",
    "batch_size=1\n",
    "data_aug=True\n",
    "type_resize=\"interpolation\" # letterboxing or interpolation\n",
    "trans=True\n",
    "bboxtipe='xy' #coco or xy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e5543-b0c0-454e-8131-d2f993241a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_traind_t=CustomDataset(csv_path_traind,target_size=new_size,normalizacion=normalize,type_resize=type_resize,augmentation=trans,bboxtipe=bboxtipe,dataroot=dataroot)\n",
    "dataloader_traind_t = DataLoader(set_traind_t, batch_size=batch_size, shuffle=False,collate_fn=collate_fn)\n",
    "#to execute transformations bboxtipe must be coco\n",
    "#prog_bar = tqdm(dataloader_traind_t, total=len(dataloader_traind_t))\n",
    "#for i, batch in enumerate(prog_bar):\n",
    "#    imagenes, bboxes = batch\n",
    "#    Visualize(imagenes[0],bboxes[0]['boxes'][0],bb=False,ruta=False,dim=False,bz=batch_size,bboxtipe=bboxtipe) #jala con batch=1\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9c088-7b9b-4223-8df5-275c1a16d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_testd_t=CustomDataset(csv_path_testd,target_size=new_size,normalizacion=normalize,type_resize=type_resize,augmentation=trans,bboxtipe=bboxtipe,dataroot=dataroot)\n",
    "dataloader_testd_t = DataLoader(set_testd_t, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)\n",
    "#for batch in dataloader_testd:\n",
    "#    imagenes, bboxes = batch\n",
    "#    Visualize(imagenes[0],bboxes[0],bb=False,ruta=False,dim=False,bz=batch_size,bboxtipe=bboxtipe) #jala con batch=1\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc426ea5-e199-41c2-9315-5995c8eef8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#originales\n",
    "set_traind=CustomDataset(csv_path_traind,target_size=new_size,normalizacion=normalize,type_resize=type_resize,augmentation=None,bboxtipe=bboxtipe,dataroot=dataroot)\n",
    "dataloader_traind = DataLoader(set_traind, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)\n",
    "#for batch in dataloader_traind:\n",
    "#    imagenes, targets = batch \n",
    "#    Visualize(imagenes[0],bboxes[0]['boxes'][0],bb=False,ruta=False,dim=False,bz=batch_size,bboxtipe=bboxtipe) #jala con batch=1\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35717ec9-c26d-4df7-b365-d8541bd5cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_testd=CustomDataset(csv_path_testd,target_size=new_size,normalizacion=normalize,type_resize=type_resize,augmentation=None,bboxtipe=bboxtipe,dataroot=dataroot)\n",
    "dataloader_testd = DataLoader(set_testd, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)\n",
    "#for batch in dataloader_testd:\n",
    "#    imagenes, bboxes = batch\n",
    "#    Visualize(imagenes[0],bboxes[0],bb=False,ruta=False,dim=False,bz=batch_size,bboxtipe=bboxtipe) #jala con batch=1\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97760e65-2b30-4137-bc09-0b6b2e3267d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43652a02-b3c1-4c79-a199-e26ca1b29c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes,pretrain=True):\n",
    "\n",
    "    if pretrain:\n",
    "        # load Faster RCNN pre-trained model\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    else:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    # get the number of input features \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # define a new head for the detector with required number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd464a0-4be8-4854-b7fb-9cc029f7a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_loader, model):\n",
    "    print('Training')\n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "    \n",
    "     # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "        train_loss_hist.send(loss_value)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c8026-9950-428a-b7f0-0a6842f8a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_data_loader, model):\n",
    "    print('Validating')\n",
    "    global val_itr\n",
    "    global val_loss_list\n",
    "    \n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "        val_loss_hist.send(loss_value)\n",
    "        val_itr += 1\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157132c9-6f7f-43a3-81d9-e5eee4a87b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(model,name):\n",
    "    train_csv = pd.DataFrame(columns=['epoch', 'loss'])\n",
    "    train_path='Modelos/'+model+'/train'+name+'.csv'\n",
    "    train_csv.to_csv(train_path, index=False)\n",
    "    model\n",
    "    train_csv = pd.DataFrame(columns=['epoch', 'loss'])\n",
    "    validation_path='Modelos/'+model+'/validation'+name+'.csv'\n",
    "    train_csv.to_csv(validation_path, index=False)\n",
    "    return train_path, validation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cab19-156b-40bf-af6a-8d6c9856ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graphs(train_path,validation_path,title_graph,name):\n",
    "    train = pd.read_csv(train_path)\n",
    "    val = pd.read_csv(validation_path)\n",
    "\n",
    "    x1 = train['epoch']\n",
    "    y1 = train['loss']\n",
    "\n",
    "    x2 = val['epoch']\n",
    "    y2 = val['loss']\n",
    "\n",
    "    plt.plot(x1, y1, marker='o', linestyle='-') \n",
    "    plt.title('train graph '+title_graph)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig('Modelos/RCNN/train'+name+'.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(x2, y2, marker='o', linestyle='-') \n",
    "    plt.title('validation graph '+title_graph)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig('Modelos/RCNN/validation'+name+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0dca9-49e5-4655-8632-9ebbac011b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, validation_path=create_csv(\"RCNN\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7058b-5544-4f74-a296-4d516a227e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tipo = \"gpus\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"There is gpu available:\", torch.cuda.is_available())\n",
    "print(\"The number of gpu's availables:\", torch.cuda.device_count())\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model =create_model(num_classes=2,pretrain=True)\n",
    "\n",
    "# Verificar si hay múltiples GPUs disponibles y usar DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# initialize the Averager class\n",
    "train_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "    \n",
    "num_epochs = 25\n",
    "\n",
    "print('Starting training with: ', tipo)\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    train_loss_hist.reset()\n",
    "    val_loss_hist.reset()  \n",
    "\n",
    "    train_loss = train(dataloader_traind,model)\n",
    "    val_loss = validate(dataloader_testd,model)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with open(train_path, mode='a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        # Write the epoch and avetage loss in the CSV\n",
    "        csvwriter.writerow([epoch, train_loss_list[epoch]])\n",
    "\n",
    "    with open(validation_path, mode='a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        #Write the epoch and avetage loss in the CSV\n",
    "        csvwriter.writerow([epoch, val_loss_list[epoch]])\n",
    "\n",
    "# Guardar el modelo entrenado al final del entrenamiento\n",
    "torch.save(model.state_dict(),'Modelos/RCNN/modelo_entrenado_orig.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f8a48-5723-45a4-b064-d514774dc14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graphs(train_path,validation_path,'original data',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052e7ce-4116-4417-96b3-1f051dfdfacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo_entrenado(ruta_modelo):\n",
    "    modelo = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)  # Carga el modelo sin pesos pre-entrenados\n",
    "    checkpoint = torch.load(ruta_modelo)\n",
    "    modelo.load_state_dict(checkpoint)  # Carga los pesos entrenados previamente\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d183c4-165f-48f0-9a47-54eea9cb5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, validation_path=create_csv(\"RCNN\",\"_orig_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90493f-5835-4952-bdc9-e4aa65649cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo = \"gpus\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"There is gpu available:\", torch.cuda.is_available())\n",
    "print(\"The number of gpu's availables:\", torch.cuda.device_count())\n",
    "\n",
    "\n",
    "# load a model pre-trained in oriinal\n",
    "ruta_modelo_entrenado = \"Modelos/RCNN/modelo_entrenado_orig.pth\"\n",
    "model = cargar_modelo_entrenado(ruta_modelo_entrenado)\n",
    "\n",
    "# Verificar si hay múltiples GPUs disponibles y usar DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# initialize the Averager class\n",
    "train_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "    \n",
    "num_epochs = 25\n",
    "\n",
    "print('Starting training with: ', tipo)\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    train_loss_hist.reset()\n",
    "    val_loss_hist.reset()  \n",
    "\n",
    "    train_loss = train(dataloader_traind_t,model)\n",
    "    val_loss = validate(dataloader_testd_t,model)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\tMissing key(s) in state_dict: \"backbone.body.conv1.weight\", \"backbone.body.bn1.weight\", \"backbone.body.bn1.bias\", \"backbone.body.bn1.running_mean\", \"backbone.body.bn1.running_var\", \"backbone.body.layer1.0.conv1.weight\", \"backbone.body.layer1.0.bn1.weight\", \"backbone.body.layer1.0.bn1.bias\", \"backbone.body.layer1.0.bn1.running_mean\", \"backbone.body.layer1.0.bn1.running_var\", \"backbone.body.layer1.0.conv2.weight\", \"backbone.body.layer1.0.bn2.weight\", \"backbone.body.layer1.0.bn2.bias\", \"backbone.body.layer1.0.bn2.running_mean\", \"backbone.body.layer1.0.bn2.running_var\", \"backbone.body.layer1.0.conv3.weight\", \"backbone.body.layer1.0.bn3.weight\", \"backbone.body.layer1.0.bn3.bias\", \"backbone.body.layer1.0.bn3.running_mean\", \"backbone.body.layer1.0.bn3.running_var\", \"backbone.body.layer1.0.downsample.0.weight\", \"backbone.body.layer1.0.downsample.1.weight\", \"backbone.body.layer1.0.downsample.1.bias\", \"backbone.body.layer1.0.downsample.1.running_mean\", \"backbone.body.layer1.0.downsample.1.running_var\", \"backbone.body.layer1.1.conv1.weight\", \"backbone.body.layer1.1.bn1.weight\", \"backbone.body.layer1.1.bn1.bias\", \"backbone.body.layer1.1.bn1.running_mean\", \"backbone.body.layer1.1.bn1.running_var\", \"backbone.body.layer1.1.conv2.weight\", \"backbone.body.layer1.1.bn2.weight\", \"backbone.body.layer1.1.bn2.bias\", \"backbone.body.layer1.1.bn2.running_mean\", \"backbone.body.layer1.1.bn2.running_var\", \"backbone.body.layer1.1.conv3.weight\", \"backbone.body.layer1.1.bn3.weight\", \"backbone.body.layer1.1.bn3.bias\", \"backbone.body.layer1.1.bn3.running_mean\", \"backbone.body.layer1.1.bn3.running_var\", \"backbone.body.layer1.2.conv1.weight\", \"backbone.body.layer1.2.bn1.weight\", \"backbone.body.layer1.2.bn1.bias\", \"backbone.body.layer1.2.bn1.running_mean\", \"backbone.body.layer1.2.bn1.running_var\", \"backbone.body.layer1.2.conv2.weight\", \"backbone.body.layer1.2.bn2.weight\", \"backbone.body.layer1.2.bn2.bias\", \"backbone.body.layer1.2.bn2.running_mean\", \"backbone.body.layer1.2.bn2.running_var\", \"backbone.body.layer1.2.conv3.weight\", \"backbone.body.layer1.2.bn3.weight\", \"backbone.body.layer1.2.bn3.bias\", \"backbone.body.layer1.2.bn3.running_mean\", \"backbone.body.layer1.2.bn3.running_var\", \"backbone.body.layer2.0.conv1.weight\", \"backbone.body.layer2.0.bn1.weight\", \"backbone.body.layer2.0.bn1.bias\", \"backbone.body.layer2.0.bn1.running_mean\", \"backbone.body.layer2.0.bn1.running_var\", \"backbone.body.layer2.0.conv2.weight\", \"backbone.body.layer2.0.bn2.weight\", \"backbone.body.layer2.0.bn2.bias\", \"backbone.body.layer2.0.bn2.running_mean\", \"backbone.body.layer2.0.bn2.running_var\", \"backbone.body.layer2.0.conv3.weight\", \"backbone.body.layer2.0.bn3.weight\", \"backbone.body.layer2.0.bn3.bias\", \"backbone.body.layer2.0.bn3.running_mean\", \"backbone.body.layer2.0.bn3.running_var\", \"backbone.body.layer2.0.downsample.0.weight\", \"backbone.body.layer2.0.downsample.1.weight\", \"backbone.body.layer2.0.downsample.1.bias\", \"backbone.body.layer2.0.downsample.1.running_mean\", \"backbone.body.layer2.0.downsample.1.running_var\", \"backbone.body.layer2.1.conv1.weight\", \"backbone.body.layer2.1.bn1.weight\", \"backbone.body.layer2.1.bn1.bias\", \"backbone.body.layer2.1.bn1.running_mean\", \"backbone.body.layer2.1.bn1.running_var\", \"backbone.body.layer2.1.conv2.weight\", \"backbone.body.layer2.1.bn2.weight\", \"backbone.body.layer2.1.bn2.bias\", \"backbone.body.layer2.1.bn2.running_mean\", \"backbone.body.layer2.1.bn2.running_var\", \"backbone.body.layer2.1.conv3.weight\", \"backbone.body.layer2.1.bn3.weight\", \"backbone.body.layer2.1.bn3.bias\", \"backbone.body.layer2.1.bn3.running_mean\", \"backbone.body.layer2.1.bn3.running_var\", \"backbone.body.layer2.2.conv1.weight\", \"backbone.body.layer2.2.bn1.weight\", \"backbone.body.layer2.2.bn1.bias\", \"backbone.body.layer2.2.bn1.running_mean\", \"backbone.body.layer2.2.bn1.running_var\", \"backbone.body.layer2.2.conv2.weight\", \"backbone.body.layer2.2.bn2.weight\", \"backbone.body.layer2.2.bn2.bias\", \"backbone.body.layer2.2.bn2.running_mean\", \"backbone.body.layer2.2.bn2.running_var\", \"backbone.body.layer2.2.conv3.weight\", \"backbone.body.layer2.2.bn3.weight\", \"backbone.body.layer2.2.bn3.bias\", \"backbone.body.layer2.2.bn3.running_mean\", \"backbone.body.layer2.2.bn3.running_var\", \"backbone.body.layer2.3.conv1.weight\", \"backbone.body.layer2.3.bn1.weight\", \"backbone.body.layer2.3.bn1.bias\", \"backbone.body.layer2.3.bn1.running_mean\", \"backbone.body.layer2.3.bn1.running_var\", \"backbone.body.layer2.3.conv2.weight\", \"backbone.body.layer2.3.bn2.weight\", \"backbone.body.layer2.3.bn2.bias\", \"backbone.body.layer2.3.bn2.running_mean\", \"backbone.body.layer2.3.bn2.running_var\", \"backbone.body.layer2.3.conv3.weight\", \"backbone.body.layer2.3.bn3.weight\", \"backbone.body.layer2.3.bn3.bias\", \"backbone.body.layer2.3.bn3.running_mean\", \"backbone.body.layer2.3.bn3.running_var\", \"backbone.body.layer3.0.conv1.weight\", \"backbone.body.layer3.0.bn1.weight\", \"backbone.body.layer3.0.bn1.bias\", \"backbone.body.layer3.0.bn1.running_mean\", \"backbone.body.layer3.0.bn1.running_var\", \"backbone.body.layer3.0.conv2.weight\", \"backbone.body.layer3.0.bn2.weight\", \"backbone.body.layer3.0.bn2.bias\", \"backbone.body.layer3.0.bn2.running_mean\", \"backbone.body.layer3.0.bn2.running_var\", \"backbone.body.layer3.0.conv3.weight\", \"backbone.body.layer3.0.bn3.weight\", \"backbone.body.layer3.0.bn3.bias\", \"backbone.body.layer3.0.bn3.running_mean\", \"backbone.body.layer3.0.bn3.running_var\", \"backbone.body.layer3.0.downsample.0.weight\", \"backbone.body.layer3.0.downsample.1.weight\", \"backbone.body.layer3.0.downsample.1.bias\", \"backbone.body.layer3.0.downsample.1.running_mean\", \"backbone.body.layer3.0.downsample.1.running_var\", \"backbone.body.layer3.1.conv1.weight\", \"backbone.body.layer3.1.bn1.weight\", \"backbone.body.layer3.1.bn1.bias\", \"backbone.body.layer3.1.bn1.running_mean\", \"backbone.body.layer3.1.bn1.running_var\", \"backbone.body.layer3.1.conv2.weight\", \"backbone.body.layer3.1.bn2.weight\", \"backbone.body.layer3.1.bn2.bias\", \"backbone.body.layer3.1.bn2.running_mean\", \"backbone.body.layer3.1.bn2.running_var\", \"backbone.body.layer3.1.conv3.weight\", \"backbone.body.layer3.1.bn3.weight\", \"backbone.body.layer3.1.bn3.bias\", \"backbone.body.layer3.1.bn3.running_mean\", \"backbone.body.layer3.1.bn3.running_var\", \"backbone.body.layer3.2.conv1.weight\", \"backbone.body.layer3.2.bn1.weight\", \"backbone.body.layer3.2.bn1.bias\", \"backbone.body.layer3.2.bn1.running_mean\", \"backbone.body.layer3.2.bn1.running_var\", \"backbone.body.layer3.2.conv2.weight\", \"backbone.body.layer3.2.bn2.weight\", \"backbone.body.layer3.2.bn2.bias\", \"backbone.body.layer3.2.bn2.running_mean\", \"backbone.body.layer3.2.bn2.running_var\", \"backbone.body.layer3.2.conv3.weight\", \"backbone.body.layer3.2.bn3.weight\", \"backbone.body.layer3.2.bn3.bias\", \"backbone.body.layer3.2.bn3.running_mean\", \"backbone.body.layer3.2.bn3.running_var\", \"backbone.body.layer3.3.conv1.weight\", \"backbone.body.layer3.3.bn1.weight\", \"backbone.body.layer3.3.bn1.bias\", \"backbone.body.layer3.3.bn1.running_mean\", \"backbone.body.layer3.3.bn1.running_var\", \"backbone.body.layer3.3.conv2.weight\", \"backbone.body.layer3.3.bn2.weight\", \"backbone.body.layer3.3.bn2.bias\", \"backbone.body.layer3.3.bn2.running_mean\", \"backbone.body.layer3.3.bn2.running_var\", \"backbone.body.layer3.3.conv3.weight\", \"backbone.body.layer3.3.bn3.weight\", \"backbone.body.layer3.3.bn3.bias\", \"backbone.body.layer3.3.bn3.running_mean\", \"backbone.body.layer3.3.bn3.running_var\", \"backbone.body.layer3.4.conv1.weight\", \"backbone.body.layer3.4.bn1.weight\", \"backbone.body.layer3.4.bn1.bias\", \"backbone.body.layer3.4.bn1.running_mean\", \"backbone.body.layer3.4.bn1.running_var\", \"backbone.body.layer3.4.conv2.weight\", \"backbone.body.layer3.4.bn2.weight\", \"backbone.body.layer3.4.bn2.bias\", \"backbone.body.layer3.4.bn2.running_mean\", \"backbone.body.layer3.4.bn2.running_var\", \"backbone.body.layer3.4.conv3.weight\", \"backbone.body.layer3.4.bn3.weight\", \"backbone.body.layer3.4.bn3.bias\", \"backbone.body.layer3.4.bn3.running_mean\", \"backbone.body.layer3.4.bn3.running_var\", \"backbone.body.layer3.5.conv1.weight\", \"backbone.body.layer3.5.bn1.weight\", \"backbone.body.layer3.5.bn1.bias\", \"backbone.body.layer3.5.bn1.running_mean\", \"backbone.body.layer3.5.bn1.running_var\", \"backbone.body.layer3.5.conv2.weight\", \"backbone.body.layer3.5.bn2.weight\", \"backbone.body.layer3.5.bn2.bias\", \"backbone.body.layer3.5.bn2.running_mean\", \"backbone.body.layer3.5.bn2.running_var\", \"backbone.body.layer3.5.conv3.weight\", \"backbone.body.layer3.5.bn3.weight\", \"backbone.body.layer3.5.bn3.bias\", \"backbone.body.layer3.5.bn3.running_mean\", \"backbone.body.layer3.5.bn3.running_var\", \"backbone.body.layer4.0.conv1.weight\", \"backbone.body.layer4.0.bn1.weight\", \"backbone.body.layer4.0.bn1.bias\", \"backbone.body.layer4.0.bn1.running_mean\", \"backbone.body.layer4.0.bn1.running_var\", \"backbone.body.layer4.0.conv2.weight\", \"backbone.body.layer4.0.bn2.weight\", \"backbone.body.layer4.0.bn2.bias\", \"backbone.body.layer4.0.bn2.running_mean\", \"backbone.body.layer4.0.bn2.running_var\", \"backbone.body.layer4.0.conv3.weight\", \"backbone.body.layer4.0.bn3.weight\", \"backbone.body.layer4.0.bn3.bias\", \"backbone.body.layer4.0.bn3.running_mean\", \"backbone.body.layer4.0.bn3.running_var\", \"backbone.body.layer4.0.downsample.0.weight\", \"backbone.body.layer4.0.downsample.1.weight\", \"backbone.body.layer4.0.downsample.1.bias\", \"backbone.body.layer4.0.downsample.1.running_mean\", \"backbone.body.layer4.0.downsample.1.running_var\", \"backbone.body.layer4.1.conv1.weight\", \"backbone.body.layer4.1.bn1.weight\", \"backbone.body.layer4.1.bn1.bias\", \"backbone.body.layer4.1.bn1.running_mean\", \"backbone.body.layer4.1.bn1.running_var\", \"backbone.body.layer4.1.conv2.weight\", \"backbone.body.layer4.1.bn2.weight\", \"backbone.body.layer4.1.bn2.bias\", \"backbone.body.layer4.1.bn2.running_mean\", \"backbone.body.layer4.1.bn2.running_var\", \"backbone.body.layer4.1.conv3.weight\", \"backbone.body.layer4.1.bn3.weight\", \"backbone.body.layer4.1.bn3.bias\", \"backbone.body.layer4.1.bn3.running_mean\", \"backbone.body.layer4.1.bn3.running_var\", \"backbone.body.layer4.2.conv1.weight\", \"backbone.body.layer4.2.bn1.weight\", \"backbone.body.layer4.2.bn1.bias\", \"backbone.body.layer4.2.bn1.running_mean\", \"backbone.body.layer4.2.bn1.running_var\", \"backbone.body.layer4.2.conv2.weight\", \"backbone.body.layer4.2.bn2.weight\", \"backbone.body.layer4.2.bn2.bias\", \"backbone.body.layer4.2.bn2.running_mean\", \"backbone.body.layer4.2.bn2.running_var\", \"backbone.body.layer4.2.conv3.weight\", \"backbone.body.layer4.2.bn3.weight\", \"backbone.body.layer4.2.bn3.bias\", \"backbone.body.layer4.2.bn3.running_mean\", \"backbone.body.layer4.2.bn3.running_var\", \"backbone.fpn.inner_blocks.0.0.weight\", \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.weight\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.weight\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.weight\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.weight\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.weight\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.weight\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.weight\", \"backbone.fpn.layer_blocks.3.0.bias\", \"rpn.head.conv.0.0.weight\", \"rpn.head.conv.0.0.bias\", \"rpn.head.cls_logits.weight\", \"rpn.head.cls_logits.bias\", \"rpn.head.bbox_pred.weight\", \"rpn.head.bbox_pred.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\", \"roi_heads.box_predictor.cls_score.weight\", \"roi_heads.box_predictor.cls_score.bias\", \"roi_heads.box_predictor.bbox_pred.weight\", \"roi_heads.box_predictor.bbox_pred.bias\". \n",
    "\tUnexpected key(s) in state_dict: \"module.backbone.body.conv1.weight\", \"module.backbone.body.bn1.weight\", \"module.backbone.body.bn1.bias\", \"module.backbone.body.bn1.running_mean\", \"module.backbone.body.bn1.running_var\", \"module.backbone.body.layer1.0.conv1.weight\", \"module.backbone.body.layer1.0.bn1.weight\", \"module.backbone.body.layer1.0.bn1.bias\", \"module.backbone.body.layer1.0.bn1.running_mean\", \"module.backbone.body.layer1.0.bn1.running_var\", \"module.backbone.body.layer1.0.conv2.weight\", \"module.backbone.body.layer1.0.bn2.weight\", \"module.backbone.body.layer1.0.bn2.bias\", \"module.backbone.body.layer1.0.bn2.running_mean\", \"module.backbone.body.layer1.0.bn2.running_var\", \"module.backbone.body.layer1.0.conv3.weight\", \"module.backbone.body.layer1.0.bn3.weight\", \"module.backbone.body.layer1.0.bn3.bias\", \"module.backbone.body.layer1.0.bn3.running_mean\", \"module.backbone.body.layer1.0.bn3.running_var\", \"module.backbone.body.layer1.0.downsample.0.weight\", \"module.backbone.body.layer1.0.downsample.1.weight\", \"module.backbone.body.layer1.0.downsample.1.bias\", \"module.backbone.body.layer1.0.downsample.1.running_mean\", \"module.backbone.body.layer1.0.downsample.1.running_var\", \"module.backbone.body.layer1.1.conv1.weight\", \"module.backbone.body.layer1.1.bn1.weight\", \"module.backbone.body.layer1.1.bn1.bias\", \"module.backbone.body.layer1.1.bn1.running_mean\", \"module.backbone.body.layer1.1.bn1.running_var\", \"module.backbone.body.layer1.1.conv2.weight\", \"module.backbone.body.layer1.1.bn2.weight\", \"module.backbone.body.layer1.1.bn2.bias\", \"module.backbone.body.layer1.1.bn2.running_mean\", \"module.backbone.body.layer1.1.bn2.running_var\", \"module.backbone.body.layer1.1.conv3.weight\", \"module.backbone.body.layer1.1.bn3.weight\", \"module.backbone.body.layer1.1.bn3.bias\", \"module.backbone.body.layer1.1.bn3.running_mean\", \"module.backbone.body.layer1.1.bn3.running_var\", \"module.backbone.body.layer1.2.conv1.weight\", \"module.backbone.body.layer1.2.bn1.weight\", \"module.backbone.body.layer1.2.bn1.bias\", \"module.backbone.body.layer1.2.bn1.running_mean\", \"module.backbone.body.layer1.2.bn1.running_var\", \"module.backbone.body.layer1.2.conv2.weight\", \"module.backbone.body.layer1.2.bn2.weight\", \"module.backbone.body.layer1.2.bn2.bias\", \"module.backbone.body.layer1.2.bn2.running_mean\", \"module.backbone.body.layer1.2.bn2.running_var\", \"module.backbone.body.layer1.2.conv3.weight\", \"module.backbone.body.layer1.2.bn3.weight\", \"module.backbone.body.layer1.2.bn3.bias\", \"module.backbone.body.layer1.2.bn3.running_mean\", \"module.backbone.body.layer1.2.bn3.running_var\", \"module.backbone.body.layer2.0.conv1.weight\", \"module.backbone.body.layer2.0.bn1.weight\", \"module.backbone.body.layer2.0.bn1.bias\", \"module.backbone.body.layer2.0.bn1.running_mean\", \"module.backbone.body.layer2.0.bn1.running_var\", \"module.backbone.body.layer2.0.conv2.weight\", \"module.backbone.body.layer2.0.bn2.weight\", \"module.backbone.body.layer2.0.bn2.bias\", \"module.backbone.body.layer2.0.bn2.running_mean\", \"module.backbone.body.layer2.0.bn2.running_var\", \"module.backbone.body.layer2.0.conv3.weight\", \"module.backbone.body.layer2.0.bn3.weight\", \"module.backbone.body.layer2.0.bn3.bias\", \"module.backbone.body.layer2.0.bn3.running_mean\", \"module.backbone.body.layer2.0.bn3.running_var\", \"module.backbone.body.layer2.0.downsample.0.weight\", \"module.backbone.body.layer2.0.downsample.1.weight\", \"module.backbone.body.layer2.0.downsample.1.bias\", \"module.backbone.body.layer2.0.downsample.1.running_mean\", \"module.backbone.body.layer2.0.downsample.1.running_var\", \"module.backbone.body.layer2.1.conv1.weight\", \"module.backbone.body.layer2.1.bn1.weight\", \"module.backbone.body.layer2.1.bn1.bias\", \"module.backbone.body.layer2.1.bn1.running_mean\", \"module.backbone.body.layer2.1.bn1.running_var\", \"module.backbone.body.layer2.1.conv2.weight\", \"module.backbone.body.layer2.1.bn2.weight\", \"module.backbone.body.layer2.1.bn2.bias\", \"module.backbone.body.layer2.1.bn2.running_mean\", \"module.backbone.body.layer2.1.bn2.running_var\", \"module.backbone.body.layer2.1.conv3.weight\", \"module.backbone.body.layer2.1.bn3.weight\", \"module.backbone.body.layer2.1.bn3.bias\", \"module.backbone.body.layer2.1.bn3.running_mean\", \"module.backbone.body.layer2.1.bn3.running_var\", \"module.backbone.body.layer2.2.conv1.weight\", \"module.backbone.body.layer2.2.bn1.weight\", \"module.backbone.body.layer2.2.bn1.bias\", \"module.backbone.body.layer2.2.bn1.running_mean\", \"module.backbone.body.layer2.2.bn1.running_var\", \"module.backbone.body.layer2.2.conv2.weight\", \"module.backbone.body.layer2.2.bn2.weight\", \"module.backbone.body.layer2.2.bn2.bias\", \"module.backbone.body.layer2.2.bn2.running_mean\", \"module.backbone.body.layer2.2.bn2.running_var\", \"module.backbone.body.layer2.2.conv3.weight\", \"module.backbone.body.layer2.2.bn3.weight\", \"module.backbone.body.layer2.2.bn3.bias\", \"module.backbone.body.layer2.2.bn3.running_mean\", \"module.backbone.body.layer2.2.bn3.running_var\", \"module.backbone.body.layer2.3.conv1.weight\", \"module.backbone.body.layer2.3.bn1.weight\", \"module.backbone.body.layer2.3.bn1.bias\", \"module.backbone.body.layer2.3.bn1.running_mean\", \"module.backbone.body.layer2.3.bn1.running_var\", \"module.backbone.body.layer2.3.conv2.weight\", \"module.backbone.body.layer2.3.bn2.weight\", \"module.backbone.body.layer2.3.bn2.bias\", \"module.backbone.body.layer2.3.bn2.running_mean\", \"module.backbone.body.layer2.3.bn2.running_var\", \"module.backbone.body.layer2.3.conv3.weight\", \"module.backbone.body.layer2.3.bn3.weight\", \"module.backbone.body.layer2.3.bn3.bias\", \"module.backbone.body.layer2.3.bn3.running_mean\", \"module.backbone.body.layer2.3.bn3.running_var\", \"module.backbone.body.layer3.0.conv1.weight\", \"module.backbone.body.layer3.0.bn1.weight\", \"module.backbone.body.layer3.0.bn1.bias\", \"module.backbone.body.layer3.0.bn1.running_mean\", \"module.backbone.body.layer3.0.bn1.running_var\", \"module.backbone.body.layer3.0.conv2.weight\", \"module.backbone.body.layer3.0.bn2.weight\", \"module.backbone.body.layer3.0.bn2.bias\", \"module.backbone.body.layer3.0.bn2.running_mean\", \"module.backbone.body.layer3.0.bn2.running_var\", \"module.backbone.body.layer3.0.conv3.weight\", \"module.backbone.body.layer3.0.bn3.weight\", \"module.backbone.body.layer3.0.bn3.bias\", \"module.backbone.body.layer3.0.bn3.running_mean\", \"module.backbone.body.layer3.0.bn3.running_var\", \"module.backbone.body.layer3.0.downsample.0.weight\", \"module.backbone.body.layer3.0.downsample.1.weight\", \"module.backbone.body.layer3.0.downsample.1.bias\", \"module.backbone.body.layer3.0.downsample.1.running_mean\", \"module.backbone.body.layer3.0.downsample.1.running_var\", \"module.backbone.body.layer3.1.conv1.weight\", \"module.backbone.body.layer3.1.bn1.weight\", \"module.backbone.body.layer3.1.bn1.bias\", \"module.backbone.body.layer3.1.bn1.running_mean\", \"module.backbone.body.layer3.1.bn1.running_var\", \"module.backbone.body.layer3.1.conv2.weight\", \"module.backbone.body.layer3.1.bn2.weight\", \"module.backbone.body.layer3.1.bn2.bias\", \"module.backbone.body.layer3.1.bn2.running_mean\", \"module.backbone.body.layer3.1.bn2.running_var\", \"module.backbone.body.layer3.1.conv3.weight\", \"module.backbone.body.layer3.1.bn3.weight\", \"module.backbone.body.layer3.1.bn3.bias\", \"module.backbone.body.layer3.1.bn3.running_mean\", \"module.backbone.body.layer3.1.bn3.running_var\", \"module.backbone.body.layer3.2.conv1.weight\", \"module.backbone.body.layer3.2.bn1.weight\", \"module.backbone.body.layer3.2.bn1.bias\", \"module.backbone.body.layer3.2.bn1.running_mean\", \"module.backbone.body.layer3.2.bn1.running_var\", \"module.backbone.body.layer3.2.conv2.weight\", \"module.backbone.body.layer3.2.bn2.weight\", \"module.backbone.body.layer3.2.bn2.bias\", \"module.backbone.body.layer3.2.bn2.running_mean\", \"module.backbone.body.layer3.2.bn2.running_var\", \"module.backbone.body.layer3.2.conv3.weight\", \"module.backbone.body.layer3.2.bn3.weight\", \"module.backbone.body.layer3.2.bn3.bias\", \"module.backbone.body.layer3.2.bn3.running_mean\", \"module.backbone.body.layer3.2.bn3.running_var\", \"module.backbone.body.layer3.3.conv1.weight\", \"module.backbone.body.layer3.3.bn1.weight\", \"module.backbone.body.layer3.3.bn1.bias\", \"module.backbone.body.layer3.3.bn1.running_mean\", \"module.backbone.body.layer3.3.bn1.running_var\", \"module.backbone.body.layer3.3.conv2.weight\", \"module.backbone.body.layer3.3.bn2.weight\", \"module.backbone.body.layer3.3.bn2.bias\", \"module.backbone.body.layer3.3.bn2.running_mean\", \"module.backbone.body.layer3.3.bn2.running_var\", \"module.backbone.body.layer3.3.conv3.weight\", \"module.backbone.body.layer3.3.bn3.weight\", \"module.backbone.body.layer3.3.bn3.bias\", \"module.backbone.body.layer3.3.bn3.running_mean\", \"module.backbone.body.layer3.3.bn3.running_var\", \"module.backbone.body.layer3.4.conv1.weight\", \"module.backbone.body.layer3.4.bn1.weight\", \"module.backbone.body.layer3.4.bn1.bias\", \"module.backbone.body.layer3.4.bn1.running_mean\", \"module.backbone.body.layer3.4.bn1.running_var\", \"module.backbone.body.layer3.4.conv2.weight\", \"module.backbone.body.layer3.4.bn2.weight\", \"module.backbone.body.layer3.4.bn2.bias\", \"module.backbone.body.layer3.4.bn2.running_mean\", \"module.backbone.body.layer3.4.bn2.running_var\", \"module.backbone.body.layer3.4.conv3.weight\", \"module.backbone.body.layer3.4.bn3.weight\", \"module.backbone.body.layer3.4.bn3.bias\", \"module.backbone.body.layer3.4.bn3.running_mean\", \"module.backbone.body.layer3.4.bn3.running_var\", \"module.backbone.body.layer3.5.conv1.weight\", \"module.backbone.body.layer3.5.bn1.weight\", \"module.backbone.body.layer3.5.bn1.bias\", \"module.backbone.body.layer3.5.bn1.running_mean\", \"module.backbone.body.layer3.5.bn1.running_var\", \"module.backbone.body.layer3.5.conv2.weight\", \"module.backbone.body.layer3.5.bn2.weight\", \"module.backbone.body.layer3.5.bn2.bias\", \"module.backbone.body.layer3.5.bn2.running_mean\", \"module.backbone.body.layer3.5.bn2.running_var\", \"module.backbone.body.layer3.5.conv3.weight\", \"module.backbone.body.layer3.5.bn3.weight\", \"module.backbone.body.layer3.5.bn3.bias\", \"module.backbone.body.layer3.5.bn3.running_mean\", \"module.backbone.body.layer3.5.bn3.running_var\", \"module.backbone.body.layer4.0.conv1.weight\", \"module.backbone.body.layer4.0.bn1.weight\", \"module.backbone.body.layer4.0.bn1.bias\", \"module.backbone.body.layer4.0.bn1.running_mean\", \"module.backbone.body.layer4.0.bn1.running_var\", \"module.backbone.body.layer4.0.conv2.weight\", \"module.backbone.body.layer4.0.bn2.weight\", \"module.backbone.body.layer4.0.bn2.bias\", \"module.backbone.body.layer4.0.bn2.running_mean\", \"module.backbone.body.layer4.0.bn2.running_var\", \"module.backbone.body.layer4.0.conv3.weight\", \"module.backbone.body.layer4.0.bn3.weight\", \"module.backbone.body.layer4.0.bn3.bias\", \"module.backbone.body.layer4.0.bn3.running_mean\", \"module.backbone.body.layer4.0.bn3.running_var\", \"module.backbone.body.layer4.0.downsample.0.weight\", \"module.backbone.body.layer4.0.downsample.1.weight\", \"module.backbone.body.layer4.0.downsample.1.bias\", \"module.backbone.body.layer4.0.downsample.1.running_mean\", \"module.backbone.body.layer4.0.downsample.1.running_var\", \"module.backbone.body.layer4.1.conv1.weight\", \"module.backbone.body.layer4.1.bn1.weight\", \"module.backbone.body.layer4.1.bn1.bias\", \"module.backbone.body.layer4.1.bn1.running_mean\", \"module.backbone.body.layer4.1.bn1.running_var\", \"module.backbone.body.layer4.1.conv2.weight\", \"module.backbone.body.layer4.1.bn2.weight\", \"module.backbone.body.layer4.1.bn2.bias\", \"module.backbone.body.layer4.1.bn2.running_mean\", \"module.backbone.body.layer4.1.bn2.running_var\", \"module.backbone.body.layer4.1.conv3.weight\", \"module.backbone.body.layer4.1.bn3.weight\", \"module.backbone.body.layer4.1.bn3.bias\", \"module.backbone.body.layer4.1.bn3.running_mean\", \"module.backbone.body.layer4.1.bn3.running_var\", \"module.backbone.body.layer4.2.conv1.weight\", \"module.backbone.body.layer4.2.bn1.weight\", \"module.backbone.body.layer4.2.bn1.bias\", \"module.backbone.body.layer4.2.bn1.running_mean\", \"module.backbone.body.layer4.2.bn1.running_var\", \"module.backbone.body.layer4.2.conv2.weight\", \"module.backbone.body.layer4.2.bn2.weight\", \"module.backbone.body.layer4.2.bn2.bias\", \"module.backbone.body.layer4.2.bn2.running_mean\", \"module.backbone.body.layer4.2.bn2.running_var\", \"module.backbone.body.layer4.2.conv3.weight\", \"module.backbone.body.layer4.2.bn3.weight\", \"module.backbone.body.layer4.2.bn3.bias\", \"module.backbone.body.layer4.2.bn3.running_mean\", \"module.backbone.body.layer4.2.bn3.running_var\", \"module.backbone.fpn.inner_blocks.0.0.weight\", \"module.backbone.fpn.inner_blocks.0.0.bias\", \"module.backbone.fpn.inner_blocks.1.0.weight\", \"module.backbone.fpn.inner_blocks.1.0.bias\", \"module.backbone.fpn.inner_blocks.2.0.weight\", \"module.backbone.fpn.inner_blocks.2.0.bias\", \"module.backbone.fpn.inner_blocks.3.0.weight\", \"module.backbone.fpn.inner_blocks.3.0.bias\", \"module.backbone.fpn.layer_blocks.0.0.weight\", \"module.backbone.fpn.layer_blocks.0.0.bias\", \"module.backbone.fpn.layer_blocks.1.0.weight\", \"module.backbone.fpn.layer_blocks.1.0.bias\", \"module.backbone.fpn.layer_blocks.2.0.weight\", \"module.backbone.fpn.layer_blocks.2.0.bias\", \"module.backbone.fpn.layer_blocks.3.0.weight\", \"module.backbone.fpn.layer_blocks.3.0.bias\", \"module.rpn.head.conv.0.0.weight\", \"module.rpn.head.conv.0.0.bias\", \"module.rpn.head.cls_logits.weight\", \"module.rpn.head.cls_logits.bias\", \"module.rpn.head.bbox_pred.weight\", \"module.rpn.head.bbox_pred.bias\", \"module.roi_heads.box_head.fc6.weight\", \"module.roi_heads.box_head.fc6.bias\", \"module.roi_heads.box_head.fc7.weight\", \"module.roi_heads.box_head.fc7.bias\", \"module.roi_heads.box_predictor.cls_score.weight\", \"module.roi_heads.box_predictor.cls_score.bias\", \"module.roi_heads.box_predictor.bbox_pred.weight\", \"module.roi_heads.box_predictor.bbox_pred.bias\". \n",
    "\n",
    "    with open(train_path, mode='a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        # Write the epoch and avetage loss in the CSV\n",
    "        csvwriter.writerow([epoch, train_loss_list[epoch]])\n",
    "\n",
    "    with open(validation_path, mode='a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        #Write the epoch and avetage loss in the CSV\n",
    "        csvwriter.writerow([epoch, val_loss_list[epoch]])\n",
    "\n",
    "# Guardar el modelo entrenado al final del entrenamiento\n",
    "torch.save(model.state_dict(),'Modelos/RCNN/modelo_entrenado_orig_aug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277002a7-327a-447b-8fc0-9bb18f72b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graphs(train_path,validation_path,'original data plus augmentation',\"_orig_aug\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
